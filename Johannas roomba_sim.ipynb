{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS238 Final project\n",
    "**This notebook contains Johanna's WIP code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment setup from Sicheng's file\n",
    "The following is copied with no changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_TRASH = 1.\n",
    "REWARD_WALLCOLLISION = -.5\n",
    "REWARD_MOVE = -.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    ### Initialization ###\n",
    "\n",
    "    def __init__(self, dim_x, dim_y, num_trash):\n",
    "        self.dim_x = dim_x # Width\n",
    "        self.dim_y = dim_y # Height\n",
    "        self.num_trash = num_trash\n",
    "        self.refresh()\n",
    "\n",
    "    def refresh(self):\n",
    "        self.init_robot()\n",
    "        self.init_trash()\n",
    "\n",
    "    # Robot position currently initialized to a random position\n",
    "    # in the 0th row\n",
    "    def init_robot(self):\n",
    "        self.robot_pos = (random.randint(0, self.dim_x-1), 0)\n",
    "\n",
    "    def init_trash(self):\n",
    "        self.trash_positions = [] # List of pairs\n",
    "        for i in range(self.num_trash):\n",
    "            # Add trash at random position\n",
    "            new_pos = (random.randint(0, self.dim_x-1), random.randint(0, self.dim_y-1))\n",
    "            while new_pos in self.trash_positions:\n",
    "                new_pos = (random.randint(0, self.dim_x-1), random.randint(0, self.dim_y-1))\n",
    "            self.trash_positions.append(new_pos)\n",
    "\n",
    "    ### Methods ###\n",
    "    \n",
    "    # Returns REWARD\n",
    "    def do_action(self, action):\n",
    "        if action == 0: # Move left\n",
    "            if self.robot_pos[0] == 0:\n",
    "                return REWARD_WALLCOLLISION\n",
    "            self.robot_pos = (self.robot_pos[0] - 1, self.robot_pos[1])\n",
    "            if self.robot_pos in self.trash_positions:\n",
    "                self.trash_positions.remove(self.robot_pos)\n",
    "                return REWARD_TRASH\n",
    "            else:\n",
    "                return REWARD_MOVE\n",
    "        elif action == 1: # Move right\n",
    "            if self.robot_pos[0] == self.dim_x-1:\n",
    "                return REWARD_WALLCOLLISION\n",
    "            self.robot_pos = (self.robot_pos[0] + 1, self.robot_pos[1])\n",
    "            if self.robot_pos in self.trash_positions:\n",
    "                self.trash_positions.remove(self.robot_pos)\n",
    "                return REWARD_TRASH\n",
    "            else:\n",
    "                return REWARD_MOVE\n",
    "        elif action == 2: # Move up\n",
    "            if self.robot_pos[1] == 0:\n",
    "                return REWARD_WALLCOLLISION\n",
    "            self.robot_pos = (self.robot_pos[0], self.robot_pos[1] - 1)\n",
    "            if self.robot_pos in self.trash_positions:\n",
    "                self.trash_positions.remove(self.robot_pos)\n",
    "                return REWARD_TRASH\n",
    "            else:\n",
    "                return REWARD_MOVE\n",
    "        elif action == 3: # Move down\n",
    "            if self.robot_pos[1] == self.dim_y-1:\n",
    "                return REWARD_WALLCOLLISION\n",
    "            self.robot_pos = (self.robot_pos[0], self.robot_pos[1] + 1)\n",
    "            if self.robot_pos in self.trash_positions:\n",
    "                self.trash_positions.remove(self.robot_pos)\n",
    "                return REWARD_TRASH\n",
    "            else:\n",
    "                return REWARD_MOVE\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def print_grid(self):\n",
    "        print(\"\\t  \", end=\"\")\n",
    "        for x in range(0, self.dim_x):\n",
    "            print(str(x) + \" \", end=\"\") # TODO this will break for numbers over 10\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"\\t\", end=\"\")\n",
    "        print(\"-\" * (self.dim_x*2 + 3))\n",
    "\n",
    "        # Print each row\n",
    "        for y in range(0, self.dim_y):\n",
    "            print(y, \"\\t|\", end=\"\")\n",
    "            for x in range(0, self.dim_x):\n",
    "                if (x, y) == self.robot_pos:\n",
    "                    print(\" R\", end=\"\")\n",
    "                elif (x, y) in self.trash_positions:\n",
    "                    print(\" t\", end=\"\")\n",
    "                else:\n",
    "                    print(\" .\", end=\"\")\n",
    "            print(\" |\")\n",
    "\n",
    "        print(\"\\t\", end=\"\")\n",
    "        print(\"-\" * (self.dim_x*2 + 3))\n",
    "\n",
    "    def print(self):\n",
    "        print(self.dim_x, \"by\", self.dim_y)\n",
    "        print(\"Robot at\", self.robot_pos)\n",
    "        print(self.num_trash, \"pieces of trash at\", self.trash_positions)\n",
    "        self.print_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def get_action(self, state):\n",
    "        return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    state = State(10, 10, 5)\n",
    "    policy = Policy()\n",
    "\n",
    "    while True:\n",
    "        command = input(\"> \")\n",
    "        if command == \"print\":          # Print out the state\n",
    "            state.print()\n",
    "        elif command == \"init\":         # Initialize the grid to new parameters\n",
    "            dim_x = int(input(\"Width: \"))\n",
    "            dim_y = int(input(\"Height: \"))\n",
    "            trash_num = int(input(\"Trash num: \"))\n",
    "            state = State(dim_x, dim_y, trash_num)\n",
    "            print(\"New grid with dimensions \", state.dim_x, \" by \", state.dim_y, \" and \", trash_num, \" pieces of trash\")\n",
    "        elif command == \"refresh\":      # Refresh the grid with the same parameters\n",
    "            state.refresh()\n",
    "        elif command == \"step\":         # Step once with the current policy\n",
    "            action = policy.get_action(state)\n",
    "            reward = state.do_action(action)\n",
    "            state.print_grid()\n",
    "            print(\"Took action\", action, \"with reward\", reward)\n",
    "        elif command == \"help\":         # Print out comamnds\n",
    "            print(\"Available commands are: 'print', 'init', 'refresh', 'step', 'help', 'quit'\")\n",
    "        elif command == \"quit\":         # Quit\n",
    "            return\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a simulation and calculate total score\n",
    "This function works and can be considered finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto(dim_x, dim_y, trash_num, steps):\n",
    "    \"\"\"\n",
    "    Creates an environment with dim_x, dim_y and trash_num.\n",
    "    Then runs a simulation with the given number of steps.\n",
    "    Prints the grid, action and reward for each step.\n",
    "    Finally, prints the total score for all steps combined.\n",
    "    \"\"\"\n",
    "    state = State(dim_x, dim_y, trash_num)\n",
    "    policy = Policy()\n",
    "    score = 0\n",
    "    \n",
    "    for step in range(steps):\n",
    "        action = policy.get_action(state)\n",
    "        reward = state.do_action(action)\n",
    "        state.print_grid()\n",
    "        score += reward\n",
    "        print(\"Took action\", action, \"with reward\", reward, \"\\n\")\n",
    "    \n",
    "    print(\"Total score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  0 1 2 3 4 5 6 7 8 9 \n",
      "\t-----------------------\n",
      "0 \t| . . . . . . t . R . |\n",
      "1 \t| . . . . . . . . . . |\n",
      "2 \t| . . . . . . . . . t |\n",
      "3 \t| . t . . . . . . t t |\n",
      "4 \t| . . . . t . . . t t |\n",
      "5 \t| t . t . . . t . . . |\n",
      "6 \t| . . . . . . t . t . |\n",
      "7 \t| . . . t . . . . . . |\n",
      "8 \t| . . t . . t . . . . |\n",
      "9 \t| . . t t . . . t . . |\n",
      "\t-----------------------\n",
      "Took action 0 with reward 1.0 \n",
      "\n",
      "\t  0 1 2 3 4 5 6 7 8 9 \n",
      "\t-----------------------\n",
      "0 \t| . . . . . . t . R . |\n",
      "1 \t| . . . . . . . . . . |\n",
      "2 \t| . . . . . . . . . t |\n",
      "3 \t| . t . . . . . . t t |\n",
      "4 \t| . . . . t . . . t t |\n",
      "5 \t| t . t . . . t . . . |\n",
      "6 \t| . . . . . . t . t . |\n",
      "7 \t| . . . t . . . . . . |\n",
      "8 \t| . . t . . t . . . . |\n",
      "9 \t| . . t t . . . t . . |\n",
      "\t-----------------------\n",
      "Took action 2 with reward -0.5 \n",
      "\n",
      "\t  0 1 2 3 4 5 6 7 8 9 \n",
      "\t-----------------------\n",
      "0 \t| . . . . . . t . . . |\n",
      "1 \t| . . . . . . . . R . |\n",
      "2 \t| . . . . . . . . . t |\n",
      "3 \t| . t . . . . . . t t |\n",
      "4 \t| . . . . t . . . t t |\n",
      "5 \t| t . t . . . t . . . |\n",
      "6 \t| . . . . . . t . t . |\n",
      "7 \t| . . . t . . . . . . |\n",
      "8 \t| . . t . . t . . . . |\n",
      "9 \t| . . t t . . . t . . |\n",
      "\t-----------------------\n",
      "Took action 3 with reward -0.1 \n",
      "\n",
      "\t  0 1 2 3 4 5 6 7 8 9 \n",
      "\t-----------------------\n",
      "0 \t| . . . . . . t . . . |\n",
      "1 \t| . . . . . . . . . R |\n",
      "2 \t| . . . . . . . . . t |\n",
      "3 \t| . t . . . . . . t t |\n",
      "4 \t| . . . . t . . . t t |\n",
      "5 \t| t . t . . . t . . . |\n",
      "6 \t| . . . . . . t . t . |\n",
      "7 \t| . . . t . . . . . . |\n",
      "8 \t| . . t . . t . . . . |\n",
      "9 \t| . . t t . . . t . . |\n",
      "\t-----------------------\n",
      "Took action 1 with reward -0.1 \n",
      "\n",
      "\t  0 1 2 3 4 5 6 7 8 9 \n",
      "\t-----------------------\n",
      "0 \t| . . . . . . t . . . |\n",
      "1 \t| . . . . . . . . . . |\n",
      "2 \t| . . . . . . . . . R |\n",
      "3 \t| . t . . . . . . t t |\n",
      "4 \t| . . . . t . . . t t |\n",
      "5 \t| t . t . . . t . . . |\n",
      "6 \t| . . . . . . t . t . |\n",
      "7 \t| . . . t . . . . . . |\n",
      "8 \t| . . t . . t . . . . |\n",
      "9 \t| . . t t . . . t . . |\n",
      "\t-----------------------\n",
      "Took action 3 with reward 1.0 \n",
      "\n",
      "Total score: 1.3\n"
     ]
    }
   ],
   "source": [
    "# Test the auto function\n",
    "auto(10, 10, 20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward search\n",
    "This is work-in-progress and still has errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a class MDP that holds information about our problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, S, T, R, γ, A):\n",
    "        self.S = S     # State space\n",
    "        self.T = T     # Transition function\n",
    "        self.R = R     # Reward function\n",
    "        self.γ = γ     # Discount factor\n",
    "        self.A = A     # Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize an MDP for our specific problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some made-up numbers\n",
    "dim_x = 5\n",
    "dim_y = 5\n",
    "trash_num = 7\n",
    "trash_positions = [(0, 0), (2, 0), (3, 2), (0, 3), (1, 3), (2, 3), (1, 4)]\n",
    "robot_pos = (2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State space\n",
    "x = np.arange(dim_x)   # [0, 1, ..., width-1]\n",
    "y = np.arange(dim_y)   # [0, 1, ..., height-1]\n",
    "S = list(itertools.product(x, y))   # [(0,0), (0,1), ..., (x-1, y-1)]\n",
    "\n",
    "# Action space\n",
    "A = np.array([0, 1, 2, 3])\n",
    "\n",
    "# Reward function\n",
    "R = np.zeros((len(S), len(A)))\n",
    "\n",
    "for s in S:\n",
    "    for a in A:\n",
    "        if s in trash_positions:\n",
    "            R[s, :] += REWARD_TRASH # Reward when leaving the state (exception)\n",
    "        if s[0] == 0: \n",
    "            if a == 0: # Move left into wall\n",
    "                R[s, a] += REWARD_WALLCOLLISION\n",
    "            else:\n",
    "                R[s, a] += REWARD_MOVE\n",
    "        elif s[0] == dim_x-1:\n",
    "            if a == 1: # Move wight into wall\n",
    "                R[s, a] += REWARD_WALLCOLLISION\n",
    "            else:\n",
    "                R[s, a] += REWARD_MOVE\n",
    "        elif s[1] == 0: \n",
    "            if a == 2: # Move up into wall\n",
    "                R[s, a] += REWARD_WALLCOLLISION\n",
    "            else:\n",
    "                R[s, a] += REWARD_MOVE\n",
    "        elif s[1] == dim_y-1:\n",
    "            if a == 3: # Move down into wall\n",
    "                R[s, a] += REWARD_WALLCOLLISION\n",
    "            else:\n",
    "                R[s, a] += REWARD_MOVE\n",
    "\n",
    "# Transition function\n",
    "T = np.zeros((len(S), len(A), len(S)))\n",
    "\n",
    "for s in S:\n",
    "    for a in A:\n",
    "        if a == 0: # Move left\n",
    "            if robot_pos[0] == 0:\n",
    "                T[s, 0, s] += 1 # Hits wall\n",
    "            else:\n",
    "                T[s, 0, (robot_pos[0] - 1, robot_pos[1])] = 1\n",
    "        if a == 1: # Move right\n",
    "            if robot_pos[0] == dim_x-1:\n",
    "                T[s, 1, s] += 1 # Hits wall\n",
    "            else:\n",
    "                T[s, 1, (robot_pos[0] + 1, robot_pos[1])] = 1\n",
    "        if a == 2: # Move up\n",
    "            if robot_pos[1] == 0:\n",
    "                T[s, 2, s] += 1 # Hits wall\n",
    "            else:\n",
    "                T[s, 2, (robot_pos[0], robot_pos[1] - 1)] = 1\n",
    "        if a == 3: # Move down\n",
    "            if robot_pos[1] == dim_y-1:\n",
    "                T[s, 3, s] += 1 # Hits wall\n",
    "            else:\n",
    "                T[s, 3, (robot_pos[0], robot_pos[1] + 1)] = 1\n",
    "    \n",
    "# Discount factor\n",
    "γ = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instantiation of class MDP\n",
    "P = MDP(S, T, R, γ, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize U as a matrix of zeros\n",
    "U = np.zeros(len(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead(P, U, s, a):\n",
    "    \"\"\"\n",
    "    Given a state-action pair and a U(s),\n",
    "    computes the Q(s, a) value.\n",
    "    \"\"\"\n",
    "    return [R(s, a) + γ * sum(T(s, a, s_prime) * U(s_prime) for s_prime in S)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardSearch:\n",
    "    def __init__(self, P, d, U):\n",
    "        self.P = P\n",
    "        self.d = d\n",
    "        self.U = U\n",
    "    \n",
    "    def __call__(self, s):\n",
    "        return forward_search(self.P, s, self.d, self.U)\n",
    "\n",
    "def forward_search(P, s, d, U):\n",
    "    \"\"\"\n",
    "    Returns the expected discounted reward for a state, \n",
    "    by finding the best action to take from that state.\n",
    "    \n",
    "    This is accomplished by comparing the immediate reward + \n",
    "    the expected discounted reward for each action and \n",
    "    picking the best action.\n",
    "    \"\"\"\n",
    "    # Base case:\n",
    "    # If d=0, return no action and u = estimated utility starting at this state\n",
    "    if d <= 0:\n",
    "        return (None, U(s))\n",
    "    \n",
    "    # Initialize 'best'\n",
    "    best = (None, -math.inf)\n",
    "    \n",
    "    # Recursive call:\n",
    "    def U_prime(s):\n",
    "        return forward_search(P, s, d-1, U)[1]\n",
    "    \n",
    "    for a in P.A:\n",
    "        u = lookahead(P, U_prime, s, a)\n",
    "        if u > best[1]:\n",
    "            best = (a, u)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sparse sampling\n",
    "This is work-in-progress and still has errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSampling:\n",
    "    def __init__(self, P, d, m, U):\n",
    "        self.P = P\n",
    "        self.d = d\n",
    "        self.m = m\n",
    "        self.U = U\n",
    "        \n",
    "    def __call__(self, s):\n",
    "        return sparse_sampling(self.P, s, self.d, self.m, self.U)\n",
    "    \n",
    "def sparse_sampling(P, s, d, m, U):\n",
    "    \"\"\"\n",
    "    Returns an approximately optimal action for a discrete problem P,\n",
    "    from current state s to depth d with m samples per action.\n",
    "    \n",
    "    The returned tuple consists of the best action a and its finite-\n",
    "    horizon expected value u.\n",
    "    \"\"\"\n",
    "    # Base case: If d=0, return no action and u = estimated \n",
    "    # utility starting at this state\n",
    "    if d <= 0:\n",
    "        return (None, U(s))\n",
    "    \n",
    "    # Initialize 'best'\n",
    "    best = (None, -math.inf)\n",
    "    \n",
    "    # Recursive call:\n",
    "    for a in P.A:\n",
    "        u = 0\n",
    "        for i in range(m):\n",
    "            s_prime = #TODO\n",
    "            r = #TODO\n",
    "            a_prime, u_prime = sparse_sampling(P, s_prime, d-1, m, U)\n",
    "            u += (r + P.γ*u_prime) / m\n",
    "            \n",
    "        if u > best[1]:\n",
    "            best = (a, u)\n",
    "    \n",
    "    return best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
